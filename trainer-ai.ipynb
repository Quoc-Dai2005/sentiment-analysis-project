{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89731196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "from underthesea import word_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {device}\")\n",
    "\n",
    "MODEL_NAME = \"vinai/phobert-large\" \n",
    "MAX_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e9c52b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang t√°ch t·ª´ (Word Segmentation)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mƒêang t√°ch t·ª´ (Word Segmentation)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mcontent_seg\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m train_df, test_df = train_test_split(df, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     27\u001b[39m train_dataset = Dataset.from_pandas(train_df[[\u001b[33m'\u001b[39m\u001b[33mcontent_seg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     20\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mƒêang t√°ch t·ª´ (Word Segmentation)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mcontent_seg\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     25\u001b[39m train_df, test_df = train_test_split(df, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     27\u001b[39m train_dataset = Dataset.from_pandas(train_df[[\u001b[33m'\u001b[39m\u001b[33mcontent_seg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\venv\\Lib\\site-packages\\underthesea\\pipeline\\word_tokenize\\__init__.py:48\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(sentence, format, use_token_normalize, fixed_words)\u001b[39m\n\u001b[32m     46\u001b[39m output = []\n\u001b[32m     47\u001b[39m num_words = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tag, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tag == \u001b[33m\"\u001b[39m\u001b[33mI-W\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_words > \u001b[32m0\u001b[39m:\n\u001b[32m     50\u001b[39m         output[-\u001b[32m1\u001b[39m] = output[-\u001b[32m1\u001b[39m] + \u001b[33mu\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + token\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "file_path = 'data/comments.csv'\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Kh√¥ng t√¨m th·∫•y file {file_path}. Vui l√≤ng ki·ªÉm tra l·∫°i!\")\n",
    "else:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "\n",
    "    df = df[['rating', 'content']].dropna()\n",
    "    df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
    "    df.dropna(subset=['rating'], inplace=True)\n",
    "\n",
    "    def map_label(rating):\n",
    "        if rating in [4, 5]: return 2\n",
    "        if rating == 3: return 1\n",
    "        return 0\n",
    "\n",
    "    df['label'] = df['rating'].apply(map_label)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "\n",
    "    print(\"ƒêang t√°ch t·ª´ (Word Segmentation)...\")\n",
    "    df['content_seg'] = df['content'].apply(lambda x: word_tokenize(str(x), format=\"text\"))\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df[['content_seg', 'label']])\n",
    "    test_dataset = Dataset.from_pandas(test_df[['content_seg', 'label']])\n",
    "\n",
    "    print(f\"ƒê√£ x·ª≠ l√Ω xong. Train: {len(train_df)} - Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1d4193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang t·∫£i Tokenizer: vinai/phobert-large...\n",
      "ƒêang m√£ h√≥a d·ªØ li·ªáu...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a641702101004352acf180450aac3b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8412042e504184b5192bd50077d9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√£ h√≥a xong!\n"
     ]
    }
   ],
   "source": [
    "print(f\"ƒêang t·∫£i Tokenizer: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content_seg\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "print(\"ƒêang m√£ h√≥a d·ªØ li·ªáu...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "print(\"M√£ h√≥a xong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e91db9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang t·∫£i Model: vinai/phobert-large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\AI\\venv\\Lib\\site-packages\\transformers\\training_args.py:1636: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ c·∫•u h√¨nh xong Trainer!\n"
     ]
    }
   ],
   "source": [
    "print(f\"ƒêang t·∫£i Model: {MODEL_NAME}...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3).to(device)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_phobert\",\n",
    "    num_train_epochs=5,             \n",
    "    per_device_train_batch_size=4,   \n",
    "    gradient_accumulation_steps=4,   \n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,     \n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    report_to=\"none\",                \n",
    "    no_cuda=False if torch.cuda.is_available() else True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"ƒê√£ c·∫•u h√¨nh xong Trainer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='25820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   19/25820 40:27 < 1023:36:07, 0.01 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN...\")\n",
    "trainer.train()\n",
    "print(\"Hu·∫•n luy·ªán ho√†n t·∫•t!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a9ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ƒêang t√≠nh to√°n v√† v·∫Ω bi·ªÉu ƒë·ªì...\")\n",
    "\n",
    "history = trainer.state.log_history\n",
    "train_loss = [x['loss'] for x in history if 'loss' in x]\n",
    "eval_f1 = [x['eval_f1_macro'] for x in history if 'eval_f1_macro' in x]\n",
    "\n",
    "preds_output = trainer.predict(tokenized_test)\n",
    "logits = preds_output.predictions\n",
    "y_true = preds_output.label_ids\n",
    "y_pred = np.argmax(logits, axis=-1)\n",
    "y_prob = torch.nn.functional.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "\n",
    "target_names = ['Negative üò°', 'Neutral üòê', 'Positive üòç']\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(train_loss, label='Train Loss', color='tab:blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(eval_f1, label='Val F1-Macro', color='tab:green', marker='o')\n",
    "plt.title('Validation F1 Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Th·ª±c t·∫ø')\n",
    "plt.xlabel('D·ª± ƒëo√°n')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "y_test_bin = label_binarize(y_true, classes=[0, 1, 2])\n",
    "n_classes = 3\n",
    "colors = cycle(['#ff7f0e', '#2ca02c', '#1f77b4'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=color, lw=2, label=f'{target_names[i]} (AUC={roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_prob[:, i])\n",
    "    plt.plot(recall, precision, color=color, lw=2, label=f'{target_names[i]}')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "max_probs = np.max(y_prob, axis=1)\n",
    "correct_indices = np.where(y_pred == y_true)[0]\n",
    "incorrect_indices = np.where(y_pred != y_true)[0]\n",
    "plt.hist(max_probs[correct_indices], bins=20, color='green', alpha=0.5, label='ƒê√∫ng')\n",
    "plt.hist(max_probs[incorrect_indices], bins=20, color='red', alpha=0.5, label='Sai')\n",
    "plt.title('Confidence Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- CLASSIFICATION REPORT ---\")\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./my-phobert-sentiment-final\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"ƒê√£ l∆∞u model th√†nh c√¥ng t·∫°i: {save_path}\")\n",
    "print(\"B·∫°n c√≥ th·ªÉ ch·∫°y file 'app.py' ƒë·ªÉ s·ª≠ d·ª•ng model n√†y!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
